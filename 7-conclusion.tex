\chapter*{Conclusion} % chapter* je necislovana kapitola
\addcontentsline{toc}{chapter}{Conclusion} % rucne pridanie do obsahu
\markboth{Conclusion}{Conclusion} % vyriesenie hlaviciek

This master thesis focused on the improvement of popular neural network model Mean teacher within a semi-supervised learning paradigm. This paradigm can have a big practical use because the pre-processing of data by labeling can be less time-consuming than in the supervised learning paradigm. We built on the original idea of consistency regularization, but brought a new way, how to keep prediction consistent. The involvement of self-organization in another model was our novel idea. We decided to use a neural network-based model Self-organizing map for this purpose. Since there were not many models that use a Self-organizing map in a similar way, we further investigated many aspects of the newly proposed model MT-SOM, which combined self-organization using a Self-organizing map and semi-supervised model Mean teacher. 

The first studied aspect was the Mean teacher model itself. We studied the derivation of the model suitable for binary classification in custom created task of distinction between images of animate and inanimate objects. The main result of this experiment was, how useful are unlabeled data in the classification task when consistency between predictions of Mean teacher networks was demanded. We focused on cases when very small portions of labeled data were included in the dataset ($8\%, 2\%, 1\%$ and $0.2\%$ which corresponded to 4\,000, 1\,000, 500 and 100 labeled images). We chose models with promising hyperparameters and the final results have shown, that in the cases, when a portion of labeled data was 2\% or less, the binary version of the Mean teacher model gets $3-20\%$ better results than the supervised baseline.

Another setup, that we studied was the use of self-organization combined with supervised learning, so the whole training set was labeled and we looked further, at how the auxiliary loss based on self-organization helped the performance of the model. We introduced a model called MLP-SOM. In this experiment, we worked with two tiny table datasets, which enabled us to train each model several times and report the mean accuracy of $20$ training runs. We experimented with hyperparameters such as model architecture, weight initialization, ramp-down type of introduced hyperparameter $\kappa$ which determine the ratio of supervised loss and self-organization based loss, and in each experiment we looked also at the maximum value of $\kappa$ hyperparameter. We achieved valuable insights about model hyperparameters from this experiment. As a result of the experiment, we have shown that the MLP-SOM model is better in performance than the supervised baseline. According to the setup, the best-performing MLP-SOM model had a mean accuracy $2-7\%$ better than the mean accuracy of the baseline.

In another part of the thesis, we inspected another aspect of the proposed MT-SOM model, which was the form of an image as the input of a Self-organizing map. We were considering two alternatives, to use the original image dataset CIFAR-10 or to achieve its feature vectors using convolution. In the experiment, we have shown, that a Self-organizing map is organized much worse in case of unmodified images on the input. However, when we used extracted feature vectors, the results qualitative looked much better, even though quantitative metrics were similar.

Finally, we connected all the aspects into the MT-SOM model and experimented with the image dataset CIFAR-10. In the task of multi-class classification with $4\,000$ labeled samples, our semi-supervised model achieved $x\%$ better accuracy than the supervised baseline, and with $1\,000$ labeled samples, our semi-supervised model achieved $2\%$ better accuracy than the supervised baseline. These results are means of $3$ training runs. 

We believe, that all these results are valuable contributions to semi-supervised learning research and can lead to a new semi-supervised model with excellent performance and possibilities of use. In the future, we suggest focusing on several areas of the model, that have not been explored yet, such as co-training of the Self-organizing map and the Mean teacher part of the model, because the changing feature vectors which are used as SOM input change and the SOM can not react to these changes, since it is pre-trained and unchanged. In case of experimenting with co-training, we suggest to focus also on different options for winner selection methods. Another research, that could be enriching is the experimentation with original Mean teacher architecture, which is approximately $10$ times bigger in number of parameters. This research could lead to a comparison of state-of-the-art Mean teacher performance with the performance of our model. This was not possible for us, because of insufficient computational power. We also suggest studying the performance of the model with other different architectures, setups and with other standard datasets to prove this concept also in other conditions.